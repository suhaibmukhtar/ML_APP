{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "891632f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1(data,data2):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    from sklearn import metrics\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    xtrain=data.iloc[:,:-1]\n",
    "    ytrain=data.iloc[:,-1]\n",
    "    xtest=data2.copy()\n",
    "    #scaling\n",
    "    scaler=StandardScaler()\n",
    "    xtrain_sc=scaler.fit_transform(xtrain)\n",
    "    xtest_sc=scaler.transform(xtest)\n",
    "    # Initialize DBSCAN\n",
    "    dbscan = DBSCAN(eps=1.2, min_samples=5)\n",
    "\n",
    "    # Fit DBSCAN to the data\n",
    "    model = dbscan.fit(xtrain_sc)\n",
    "    labels = model.labels_\n",
    "    sample_cores = np.zeros_like(labels, dtype=bool)\n",
    "    sample_cores[model.core_sample_indices_] = True\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    silhouette = silhouette_score(xtrain_sc, labels)\n",
    "    return n_clusters,silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7105aa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(data,data2):\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "    lr=LogisticRegression()\n",
    "    x_train=data.iloc[:,:-1] #all columns except last column\n",
    "    y_train=data.iloc[:,-1] #only last column\n",
    "    x_test=data2\n",
    "    scaler=StandardScaler()\n",
    "    x_train_sc=scaler.fit_transform(x_train)\n",
    "    #transform only on test data\n",
    "    x_test_sc=scaler.transform(x_test)\n",
    "    lr.fit(xtrain_sc,y_train)\n",
    "    y_train_pred=lr.predict(xtrain_sc)\n",
    "    train_accuracy=accuracy_score(y_train,y_train_pred)\n",
    "    y_pred=lr.predict(xtest_sc)\n",
    "    return train_accuracy,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6164785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3(data):\n",
    "    data.position=data.position.apply(lambda x:'inside' if x=='Inside' or x=='inside' else 'outside')\n",
    "    data.date=data.date.astype(str)\n",
    "    data.time=data.time.astype(str)\n",
    "    data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'])\n",
    "    data.sort_values(by=['location', 'activity', 'datetime'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data['duration'] = data.groupby(['location', 'activity','position'])['datetime'].diff().fillna(pd.Timedelta(seconds=0))\n",
    "    data['duration_seconds'] = data['duration'].dt.total_seconds()\n",
    "    data['total_duration_seconds'] = data.groupby(['date', 'location', 'position'])['duration_seconds'].transform('sum')\n",
    "    picking_placing_data = data[data['activity'].isin(['picked', 'placed'])]\n",
    "\n",
    "    # Group by date and activity, then count occurrences\n",
    "    activity_counts = picking_placing_data.groupby(['date', 'activity']).size().reset_index(name='count')\n",
    "    activity_counts=pd.DataFrame(activity_counts)\n",
    "    return data,activity_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba267e5",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f591eaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(           date      time  sensor location  number activity position  \\\n",
       " 0    2024-01-17  10:45:20     1.0       A1       1   picked   inside   \n",
       " 1    2024-01-18  10:12:30     1.0       A1       2   picked   inside   \n",
       " 2    2024-01-18  11:38:00     0.0       A1       2   picked   inside   \n",
       " 3    2024-01-16  00:17:20     1.0       A1       1   placed   inside   \n",
       " 4    2024-01-16  01:00:20     1.0       A1       1   placed   inside   \n",
       " ..          ...       ...     ...      ...     ...      ...      ...   \n",
       " 170  2024-01-16  00:31:20     1.0       A9       1   placed   inside   \n",
       " 171  2024-01-16  01:14:20     0.0       A9       1   placed   inside   \n",
       " 172  2024-01-16  11:23:20     1.0       A9       1   placed   inside   \n",
       " 173  2024-01-17  11:04:20     0.0       A9       2   placed   inside   \n",
       " 174  2024-01-18  15:27:30     1.0       A9       1   placed   inside   \n",
       " \n",
       "     location.1            datetime        duration  duration_seconds  \\\n",
       " 0           A1 2024-01-17 10:45:20 0 days 00:00:00               0.0   \n",
       " 1           A1 2024-01-18 10:12:30 0 days 23:27:10           84430.0   \n",
       " 2           A1 2024-01-18 11:38:00 0 days 01:25:30            5130.0   \n",
       " 3           A1 2024-01-16 00:17:20 0 days 00:00:00               0.0   \n",
       " 4           A1 2024-01-16 01:00:20 0 days 00:43:00            2580.0   \n",
       " ..         ...                 ...             ...               ...   \n",
       " 170         A9 2024-01-16 00:31:20 0 days 00:00:00               0.0   \n",
       " 171         A9 2024-01-16 01:14:20 0 days 00:43:00            2580.0   \n",
       " 172         A9 2024-01-16 11:23:20 0 days 10:09:00           36540.0   \n",
       " 173         A9 2024-01-17 11:04:20 0 days 23:41:00           85260.0   \n",
       " 174         A9 2024-01-18 15:27:30 1 days 04:23:10          102190.0   \n",
       " \n",
       "      total_duration_seconds  \n",
       " 0                       0.0  \n",
       " 1                  276330.0  \n",
       " 2                  276330.0  \n",
       " 3                   35590.0  \n",
       " 4                   35590.0  \n",
       " ..                      ...  \n",
       " 170                 39120.0  \n",
       " 171                 39120.0  \n",
       " 172                 39120.0  \n",
       " 173                 85260.0  \n",
       " 174                102190.0  \n",
       " \n",
       " [175 rows x 12 columns],\n",
       "          date activity  count\n",
       " 0  2024-01-16   picked     40\n",
       " 1  2024-01-16   placed     40\n",
       " 2  2024-01-17   picked     10\n",
       " 3  2024-01-17   placed      9\n",
       " 4  2024-01-18   picked     37\n",
       " 5  2024-01-18   placed     39)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data3=pd.read_excel('rawdata.xlsx')\n",
    "task3(data3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c7de4",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df556c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9770896821941663,\n",
       " array(['B74', 'A10', 'B65', ..., 'B69', 'A38', 'A80'], dtype=object))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data\n",
    "data=pd.read_excel('train.xlsx')\n",
    "#Testing data\n",
    "data2=pd.read_excel('test.xlsx')\n",
    "task2(data,data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42f910",
   "metadata": {},
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8330f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 0.6562839325746511)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training data\n",
    "data=pd.read_excel('train.xlsx')\n",
    "#Testing data\n",
    "data2=pd.read_excel('test.xlsx')\n",
    "task1(data,data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a522c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
